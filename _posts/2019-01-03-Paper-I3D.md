---
layout: post
title:  "Paper | Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"
date:   2018-01-03 21:24:01 +0800
categories: Computer Vision
tag: Paper
---

* content
{:toc}


### 概述
  - 论文主要贡献是提出了Inflated 3D conv，为了应对视频分类领域数据集缺乏，避免之前只能从头在小数据上训练的囧境，文章利用I3D将在Imagenet上训练成功的经典模型迁移学习到video数据集上，并借助two-stream结构实现了目前最好的效果，80.9%(HMDB)和98.0%(UCF-101)
### Kinetics
  - pretrained 的好处： 拿ImageNet来说明
  - datasets 400 human action classes with more than 400 examples for each class. 
  - 由于之前数据集太小，早期的3D ConvNets都很浅，只到8层
### (ConvNet+LSTM, C3D, Two-Stream) pretrained on ImageNet

![model2](/Users/sky/Documents/leeskyed.github.io/image/I3D/model2.jpg)

  - **ConvNet+LSTM**
    - ConvNet：图像分类网络的高性能使得这个网络具有吸引力，它尝试以尽可能小变化的视频来不断重用他们，通过使用它们独立地从每个帧提取特征然后再在整个视频中pooling他们的预测；忽略了temporal structure
    - 加入LSTM：捕捉时间序列和长期依赖；把LSTM with BN放在最后一个平均池化层。
  - **3D-ConvNet**
    - 由于卷积核的维度+1，参数比2D ConvNet大得多，导致更难训练，层数比较浅
    - 不能利用到ImageNet的pre-training的benefits，只能从头训练
    - 改进：small variation of C3D
      - 8 conv layers; 5 pooling layers; 2 fc layers at the top
      - 16-frame clips with 112*112-pixel crops
      - batch-normalization after all conv and fc layers
      - using temporal stride of 2 instead of 1, which reduces memory footprint and allows for bigger batches
  - **Two-Stream(Optical flow)**
    - LSTMs 不能捕捉fine low-level motion，同时需要通过多帧unrolling整个网络来得到反向传播
    - original two-stream: 利用短的视频段来建模，用每个clip的预测分数平均的方式，输入为一张RGB图像和10张optical flow(5 in individual channels, horizontal and vertical)
    - 3D fused network: 在最后一层用3d conv将spatial和flow特征进行融合
### Two-Stream Inflated 3D convNets

![model](/Users/sky/Documents/leeskyed.github.io/image/I3D/model.jpg)

  - 利用ImageNet的预训练模型，同时利用3d conv来提取RGB stream的temporal feature，最后再利用optical-flow stream来提升网络性能，最后融合
  - How 3D ConvNets can benefit from ImageNet 2D ConvNet design and params:
    - Inflating the 3D ConvNets into 3D
    - Bootstrapping 3D filters from 2D filters
      - 给预训练的2d conv增加temporal维度，从 N\*N 到 N\* N\*N ，具体实现是将 N\*N的filter重复复制N遍并归一化，基于短期内时间不变形的假设。
      - 池化操作怎么膨胀？stride怎么选？主要依赖感受野尺寸，如果图像水平方向和竖直方向相等，那么stride也相等，而且越深的特征感受野越大。但是考虑到时间因素，对称感受野不是必须的，这主要还是依赖帧率和图片大小。时间相对于空间变化过快，将合并不同object的边信息，过慢将不能捕捉场景变化。
    - Pacing receptive ﬁeld growth in space, time and net- work depth.
    - Two 3D Streams.
      - with one I3D network trained on RGB inputs, and another on ﬂow inputs which carry optimized, smooth ﬂow information.
      - 尽管3D conv能捕捉motion信息，但是与光流优化的方式和效果还是不一样。
### 实现细节：
  - 除了C3D之外都用了以ImageNet做预训练的Inveption-V1作为基础模型。
  - 除了最后一个卷积层都加了BN和RELU
  - Data augmentation both spatially and temporally; loop shorter video to satisfy input interface; random left-right flipping; 
  - 相比起C3D的16frames，输入的帧数为64 frames，可能在时间域上特征更明显
### 结果：

![result1](/Users/sky/Documents/leeskyed.github.io/image/I3D/result1.jpg)

  - 在ImageNet上训练后迁移到Kinetics和直接在Kinetics上对比，迁移后效果更好，说明RGB流起的作用更大。整体上I3D模型参数更少，更深，训练输入在时间和空间维度上都比C3D大。
